---
title: YOLOv1算法
date: 2022-08-12 13:01:25
tags: 深度学习
categories: 深度学习
cover: /images/文章图片/YOLO/001.jpg
excerpt: 基于深度学习的目标检测
--- 
### 基于深度学习的目标检测

普通的深度学习监督算法主要用来做分类，分类的目标是要识别出图中所示是一只猫。而在ILSVRC（ImageNet Large Scale Visual Recognition Challenge）竞赛以及实际的应用中，还包括目标定位和目标检测等任务。其中目标定位不仅仅要识别出来是什么物体（即分类），而且还要预测物体的位置，位置一般用边框（bounding box）标记。而目标检测实质是多目标的定位，即要在图片中定位多个目标物体，包括分类和定位。

简单来说，分类，定位和检测的区别如下：
1.分类：是什么？
2.定位：在哪里？是什么？（单个目标）
3.检测：在哪里？分别是什么？（多个目标）

目标检测对于人类来说并不困难，通过对图片中不同颜色模块的感知很容易定位并分类出其中的目标物体，但对于计算机来说，面对的是RGB像素矩阵，很难从图像中直接得到“狗”和“猫”这样的抽象概念并定位其位置，再加上有时候多个物体和杂乱的背景混杂在一起，使得目标检测更加困难。在传统视觉领域，目标检测就是一个非常热门的研究方向，一些特定目标的检测，比如人脸检测和行人检测已经有非常成熟的技术了。普通的目标检测也有过很多的尝试，但是效果总是差强人意。

传统的目标检测一般使用滑动窗口的框架，主要包括三个步骤：
1.利用不同尺寸的滑动窗口框住图中的某一部分作为候选区域；
2.提取候选区域相关的视觉特征。比如人脸检测常用的Harr特征；行人检测和普通目标检测常用的HOG特征等；
3.利用分类器进行识别，比如常用的SVM模型

传统的目标检测中，多尺度形变部件模型DPM（Deformable Part Model）是出类拔萃的，连续获得VOC（Visual Object Class）2007到2009的检测冠军，DPM把物体看成多个组成部分（比如人脸的鼻子、嘴巴等），用部件间的关系来描述物体，这个特性非常符合自然界很多物体的非刚体特征。DPM可以看做是HOG+SVM的扩展，很好的继承了两者的优势。在人脸检测、行人检测等任务上取得了不错的效果，但是DPM相对复杂，检测速度也很慢，从而也出现了很多改进的方法。正当大家热火朝天的改进DPM性能的时候，基于深度学习的目标检测横空出世，迅速盖过了DPM的风头，很多之前研究传统目标检测算法的研究者也开始转向深度学习。

基于深度学习的目标检测发展起来以后，其效果也一直难以突破。2013年R-CNN诞生了，VOC2007测试集的mAP被提升至48%,2014年通过修改网络结构又飙升至66%，同时ILSVRC2013测试集的mAP也被提升至31.4%

R-CNN是Region-based Convolutional Neural Networks的缩写，中文翻译是基于区域的卷积神经网络，是一种结合区域提名（Region Proposal）和卷积神经网络（CNN）的目标检测方法。R-CNN是第一个真正可以工业级应用的解决方案，这也和深度学习本身的发展类似，神经网络、卷积网络都不是什么新概念，但在本世纪突然真正变得可行，而一旦可行之后再迅猛发展也不足为奇了。

R-CNN这个领域目前研究非常活跃，先后出现了R-CNN，SPP-Net，Fast R-CNN，Faster R-CNN，R-FCN，YOLO，SSD等研究。这些创新工作其实很多时候是把一些传统视觉领域的方法和深度学习结合起来了，比如选择性搜索（Selective Search）和图像金字塔（Pyramid）等

深度学习相关的目标检测方法大致可以分为两派：
1.基于区域提名的：如R-CNN，SPP-Net，Fast R-CNN，Faster R-CNN，R-FCN；
2.端到端（End-to-End）：无需区域提名的，如YOLO，SSD

目前来说，基于区域提名的方法依然占据上风，但端到端的方法速度上优势明显，后续的发展拭目以待。

端到端的方法

本节介绍端到端（End-to-End）的目标检测方法，这些方法无需区域提名，包括YOLO和SSD

#### 1.2.1 YOLO
YOLO的全拼是You Only Look Once，顾名思义就是只看一次，进一步把目标判定和目标识别合二为一，所以识别的性能有了很大的提升，达到每秒45帧，而在快速版的YOLO（Fast YOLO，卷积层更少）中，可以达到每秒155帧。

针对一张图片，YOLO的处理步骤为：
1.把输入图片缩放到448*448大小
2.运行卷积网络；
3.对模型置信度卡阈值，得到目标位置和类别

YOLO将448*448大小的图切成S*S的网格，目标中心点所在的搁置负责该目标的相关检测，每个网络预测B个边框及其置信度，以及C种类别的概率。YOLO中S = 7 ，B = 2，C取决于数据集中物体类别数量，比如VOC数据集就是C = 20，对于VOC数据集来说，YOLO就是把图片统一缩放到448*448，然后每张图片平均划分为7*7=49个小格子，每个格子预测2个矩形框及其置信度，以及20中类别的概率。

YOLO简化了真个目标检测的流程，速度提升也很大，但是YOLO还是有不少可以改进的地方，比如S*S的网格就是一个比较启发式的策略，如果两个小目标同时落入一个格子中，模型也只能预测一个；另一个问题是Loss函数对不同大小的bbox未做区分。
